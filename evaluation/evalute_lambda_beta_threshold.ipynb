{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    " \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the LLM models and the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "openai_api_key = \"##\"\n",
    "\n",
    "openai_llm_model = ChatOpenAI(\n",
    "    api_key = openai_api_key,\n",
    "    model=\"o3-mini\",\n",
    "    #temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "openai_embeddings_model = OpenAIEmbeddings(\n",
    "    api_key = openai_api_key ,\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atom.utils import LangchainOutputParser\n",
    "\n",
    "lg = LangchainOutputParser(llm_model=openai_llm_model, embeddings_model=openai_embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_result = pd.read_excel(\"similar_entities.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"label_embeddings\"] = list(await lg.calculate_embeddings(text=list(df_result['label'])))\n",
    "df_result[\"name_embeddings\"] = list(await lg.calculate_embeddings(text=list(df_result['name'])))\n",
    "df_result[\"label2_embeddings\"] = list(await lg.calculate_embeddings(text=list(df_result['label2'])))\n",
    "df_result[\"name2_embeddings\"] = list(await lg.calculate_embeddings(text=list(df_result['name2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_rowwise_similarity(df, lambda_, beta):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity for each row using the formula:\n",
    "    lambda * name_embeddings + beta * label_embeddings.\n",
    "    \n",
    "    The similarity is computed between:\n",
    "    (lambda * name_embeddings + beta * label_embeddings) and \n",
    "    (lambda * name2_embeddings + beta * label2_embeddings).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with embedding columns.\n",
    "    lambda_ (float): Weight for name embeddings.\n",
    "    beta (float): Weight for label embeddings.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The input dataframe with an additional column 'cosine_similarity'.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_similarity(row):\n",
    "        # Extract embeddings for the current row\n",
    "        emb1 = lambda_ * np.array(row['name_embeddings']) + beta * np.array(row['label_embeddings'])\n",
    "        emb2 = lambda_ * np.array(row['name2_embeddings']) + beta * np.array(row['label2_embeddings'])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        return similarity\n",
    "\n",
    "    # Apply function to each row and store the result\n",
    "    df['cosine_similarity'] = df.apply(compute_similarity, axis=1)\n",
    "    \n",
    "    return df[\"cosine_similarity\"].mean()\n",
    "\n",
    "# Example usage:\n",
    "# df = compute_rowwise_similarity(df, lambda_=0.5, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lambda(df, delta=0.1):\n",
    "    \"\"\"\n",
    "    Perform grid search over lambda values to find the best lambda that maximizes cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    delta (float): Step size for lambda search (default 0.1).\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Best lambda value and the corresponding max cosine similarity.\n",
    "    \"\"\"\n",
    "    best_lambda = None\n",
    "    max_similarity = -1\n",
    "\n",
    "    # Search over lambda values from 0 to 1 with step size delta\n",
    "    lambda_values = np.arange(0, 1.1, delta)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for lambda_ in lambda_values:\n",
    "        beta = 1 - lambda_\n",
    "        avg_similarity = compute_rowwise_similarity(df, lambda_, beta)\n",
    "        results.append((lambda_, avg_similarity))\n",
    "        \n",
    "        if avg_similarity > max_similarity:\n",
    "            max_similarity = avg_similarity\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda, max_similarity, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8,\n",
       " 0.7299654380635093,\n",
       " [(0.0, 0.4221890359101122),\n",
       "  (0.05, 0.4404743611295795),\n",
       "  (0.1, 0.46084377738337806),\n",
       "  (0.15000000000000002, 0.4832447048951166),\n",
       "  (0.2, 0.5075028826554425),\n",
       "  (0.25, 0.5332945762841813),\n",
       "  (0.30000000000000004, 0.5601289623452876),\n",
       "  (0.35000000000000003, 0.5873495373517935),\n",
       "  (0.4, 0.6141624231650046),\n",
       "  (0.45, 0.6396947374186861),\n",
       "  (0.5, 0.6630779630964563),\n",
       "  (0.55, 0.6835420188460816),\n",
       "  (0.6000000000000001, 0.7004998042446285),\n",
       "  (0.65, 0.7136030360717724),\n",
       "  (0.7000000000000001, 0.722758475944278),\n",
       "  (0.75, 0.7281054430509765),\n",
       "  (0.8, 0.7299654380635093),\n",
       "  (0.8500000000000001, 0.7287791515671717),\n",
       "  (0.9, 0.7250448586993008),\n",
       "  (0.9500000000000001, 0.7192674908976456),\n",
       "  (1.0, 0.7119223179046161),\n",
       "  (1.05, 0.7034330257978003)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lambda(df=df_result, delta=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(row, lambda_=0.8):\n",
    "        # Extract embeddings for the current row\n",
    "        emb1 = lambda_ * np.array(row['name_embeddings']) + (1-lambda_) * np.array(row['label_embeddings'])\n",
    "        emb2 = lambda_ * np.array(row['name2_embeddings']) + (1-lambda_) * np.array(row['label2_embeddings'])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        return similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-itext2kg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
